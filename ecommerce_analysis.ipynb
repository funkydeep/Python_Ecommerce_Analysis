# E-commerce Analytics: Step-by-step, human-style notebook

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
customers = pd.read_csv("ecommerce_customers_sample.csv")
products = pd.read_csv("ecommerce_products_sample.csv")
orders = pd.read_csv("ecommerce_orders_sample.csv")

# ðŸ‘€ 1. EDA: Orders per customer
orders_per = orders.groupby('customer_id').size()
plt.hist(orders_per, bins=range(1,5), rwidth=0.8, edgecolor='black')
plt.title("Distribution of Orders per Customer")
plt.xlabel("Number of Orders")
plt.ylabel("Customers")
plt.show()

# ðŸ•µï¸â€â™‚ï¸ 2. Best Selling Products
best_selling = orders.groupby('product_id').size().reset_index(name='order_count')
product_map = products.set_index('product_id')['name']
best_selling['product_name'] = best_selling['product_id'].map(product_map)
print('Best Selling Products:\n', best_selling.sort_values('order_count', ascending=False))

# ðŸ“Š 3. RFM Segmentation
orders['order_date'] = pd.to_datetime(orders['order_date'])
latest = orders['order_date'].max()
rfm = orders.groupby('customer_id').agg({
    'order_date': lambda x: (latest - x.max()).days,
    'order_id': 'count',
    'product_id': lambda x: pd.merge(x.to_frame(), products, left_on='product_id', right_on='product_id')['price'].sum()
}).reset_index()
rfm.columns = ['customer_id','recency','frequency','monetary']

print('\nSample RFM Table:')
print(rfm)

# ðŸ’¡ 4. Dummy churn: Recency > median = churned
import numpy as np
rfm['churned'] = (rfm['recency'] > rfm['recency'].median()).astype(int)

# 5. (Optional simple churn model - bonus for real-world feel)
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

features = rfm[['recency','frequency','monetary']]
target = rfm['churned']
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)
logreg = LogisticRegression()
logreg.fit(X_train, y_train)
y_pred = logreg.predict(X_test)
print('\nConfusion Matrix:\n', confusion_matrix(y_test, y_pred))
